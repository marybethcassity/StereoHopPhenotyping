{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwpKD4umYece"
      },
      "source": [
        "Based off of https://colab.research.google.com/drive/1_t3KvF3qg4IJfEhTuftFI1GSlscapNgf?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaOBlN8xYvVr",
        "outputId": "b4484ca8-0918-4d4d-c965-4353be035745"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.9/dist-packages (4.27.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.2)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sUa0JqfCYecf"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "\n",
        "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
        "from transformers import AdamW\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from tqdm.notebook import tqdm\n",
        "from PIL import Image\n",
        "import os\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import albumentations as aug\n",
        "import numpy as np\n",
        "import glob\n",
        "import re\n",
        "import matplotlib.pyplot as plt "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WicQfvQEaZ76",
        "outputId": "b1062d3a-181b-4f76-d57c-fcdda11a27c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GKYODyYLYecg"
      },
      "outputs": [],
      "source": [
        "numbers = re.compile(r'(\\d+)')\n",
        "\n",
        "def numericalSort(value):\n",
        "    parts = numbers.split(value)\n",
        "    parts[1::2] = map(int, parts[1::2])\n",
        "    return parts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "9pSQzO_pYecg"
      },
      "outputs": [],
      "source": [
        "# Define your transformations\n",
        "image_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "mask_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "  \n",
        "])\n",
        "\n",
        "class SemanticSegmentationDataset(Dataset):\n",
        "    def __init__(self, image_paths, mask_paths, image_transform=image_transform, mask_transform=mask_transform):\n",
        "        self.image_paths = image_paths\n",
        "        self.mask_paths = mask_paths\n",
        "        self.image_transform = image_transform\n",
        "        self.mask_transform = mask_transform\n",
        "      \n",
        "        assert len(self.image_paths) == len(self.mask_paths), \"Number of masks and images does not match :(\"\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # Load image and mask\n",
        "        image = Image.open(self.image_paths[idx])\n",
        "        image = image.convert('RGB')\n",
        "        \n",
        "        mask = Image.open(self.mask_paths[idx])\n",
        "        mask = image.convert('L')\n",
        "\n",
        "         # Apply transformations\n",
        "        if self.image_transform:\n",
        "             image = self.image_transform(image)  \n",
        "             mask = self.mask_transform(mask)   \n",
        "\n",
        "        feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)\n",
        "        image_features = feature_extractor(image)\n",
        "        image_tensor = torch.tensor(image_features.pixel_values)\n",
        "        image_tensor = torch.squeeze(image_tensor, dim = 0)\n",
        "        \n",
        "        mask = torch.squeeze(mask, dim =0)\n",
        "        mask = mask.long()\n",
        "        \n",
        "        #print(mask.shape)\n",
        "        #print(image_tensor.shape)\n",
        "\n",
        "        return image_tensor, mask\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmjvj7DxYecg",
        "outputId": "6f5fbf18-0222-40d1-c9d9-cccee6e9e020"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training examples: 544\n",
            "Number of validation examples: 136\n"
          ]
        }
      ],
      "source": [
        "image_path = '/content/drive/MyDrive/Hops/512/Input'\n",
        "mask_path =  '/content/drive/MyDrive/Hops/512/Target'\n",
        "\n",
        "images = sorted(glob.glob(image_path + '/*.png'), key = numericalSort)\n",
        "masks = sorted(glob.glob(mask_path + '/*.png'), key = numericalSort)\n",
        "\n",
        "dataset = SemanticSegmentationDataset(images, masks, image_transform=image_transform, mask_transform=mask_transform)\n",
        "\n",
        "test_split = 0.2\n",
        "shuffle_dataset = True\n",
        "random_seed = 42\n",
        "\n",
        "dataset_size = len(dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(test_split * dataset_size))\n",
        "\n",
        "train_indices, test_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "test_sampler = SubsetRandomSampler(test_indices)\n",
        "\n",
        "train_dataloader = DataLoader(dataset, batch_size=4, sampler=train_sampler)\n",
        "test_dataloader = DataLoader(dataset, batch_size=4, sampler=test_sampler)\n",
        "\n",
        "print(\"Number of training examples:\", len(train_sampler))\n",
        "print(\"Number of validation examples:\", len(test_sampler))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMlQkZtnYech",
        "outputId": "90e43198-4423-4083-afaf-780dac2f72bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
            "  warnings.warn(\n",
            "<ipython-input-5-3a2dce47686b>:38: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
            "  image_tensor = torch.tensor(image_features.pixel_values)\n"
          ]
        }
      ],
      "source": [
        "batch = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCXE4wwoYech",
        "outputId": "c4a38825-0e33-430b-ff82-955e0527bc09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at nvidia/mit-b5 were not used when initializing SegformerForSemanticSegmentation: ['classifier.weight', 'classifier.bias']\n",
            "- This IS expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing SegformerForSemanticSegmentation from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/mit-b5 and are newly initialized: ['decode_head.linear_c.0.proj.bias', 'decode_head.batch_norm.weight', 'decode_head.linear_c.1.proj.bias', 'decode_head.linear_c.3.proj.bias', 'decode_head.linear_c.1.proj.weight', 'decode_head.linear_c.2.proj.weight', 'decode_head.batch_norm.running_mean', 'decode_head.classifier.weight', 'decode_head.batch_norm.bias', 'decode_head.linear_c.2.proj.bias', 'decode_head.linear_c.3.proj.weight', 'decode_head.linear_fuse.weight', 'decode_head.classifier.bias', 'decode_head.batch_norm.num_batches_tracked', 'decode_head.batch_norm.running_var', 'decode_head.linear_c.0.proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "id2label = {0: 'background', 1: 'foreground'}\n",
        "label2id = {'background': 0, 'foreground': 1}\n",
        "\n",
        "\n",
        "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\", ignore_mismatched_sizes=True,\n",
        "                                                         num_labels=len(id2label), id2label=id2label, label2id=label2id,\n",
        "                                                         reshape_last_stage=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9j3yqItYech",
        "outputId": "acb5e5b1-fc7f-4645-d049-9a909aedb725"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Initialized!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "optimizer = AdamW(model.parameters(), lr=0.00006)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(\"Model Initialized!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149,
          "referenced_widgets": [
            "3308bc33b63b4b0892ec7040e82d8a23",
            "6301d3e1abe34dcaa735069e598fb47c",
            "4beefa2c38f44908ae3e7899280c0a36",
            "fb344879a44442cf919a26fbc9eb2d41",
            "c296e0cdf85f453d837dd3a23710a792",
            "6ec753f5b60e4664aac81ceaead5eefd",
            "0742a8befd4b469a9119078aa6a8ff99",
            "a2bdd1e2a60b4a09840d6d1aebebc7e5",
            "f8b096dc421f4b2aa87b0d729ee7afa2",
            "7f370e11bf4a4b688a5add56c2f776e2",
            "a5048b6496af40b3aa4d45924402d3f8"
          ]
        },
        "id": "c-NNuloeYech",
        "outputId": "32ed174b-1394-43d9-f122-31b90461ad65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/136 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3308bc33b63b4b0892ec7040e82d8a23"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/models/segformer/feature_extraction_segformer.py:28: FutureWarning: The class SegformerFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use SegformerImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "for epoch in range(1,11):  # loop over the dataset multiple times\n",
        "    print(\"Epoch:\", epoch)\n",
        "    pbar = tqdm(train_dataloader)\n",
        "    accuracies = []\n",
        "    losses = []\n",
        "    val_accuracies = []\n",
        "    val_losses = []\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(pbar):\n",
        "        # get the inputs;\n",
        "        pixel_values = batch[0].to(device)\n",
        "        \n",
        "        labels = batch[1].to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "\n",
        "        # evaluate\n",
        "        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "        predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "        mask = (labels != 255) # we don't include the background class in the accuracy calculation\n",
        "        pred_labels = predicted[mask].detach().cpu().numpy()\n",
        "        true_labels = labels[mask].detach().cpu().numpy()\n",
        "        accuracy = accuracy_score(pred_labels, true_labels)\n",
        "        loss = outputs.loss\n",
        "        accuracies.append(accuracy)\n",
        "        losses.append(loss.item())\n",
        "        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})\n",
        "\n",
        "        # backward + optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    # else:\n",
        "    #     model.eval()\n",
        "    #     with torch.no_grad():\n",
        "    #         for idx, batch in enumerate(valid_dataloader):\n",
        "    #             pixel_values = batch[\"pixel_values\"].to(device)\n",
        "    #             labels = batch[\"labels\"].to(device)\n",
        "\n",
        "    #             outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "    #             upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
        "    #             predicted = upsampled_logits.argmax(dim=1)\n",
        "\n",
        "    #             mask = (labels != 255) # we don't include the background class in the accuracy calculation\n",
        "    #             pred_labels = predicted[mask].detach().cpu().numpy()\n",
        "    #             true_labels = labels[mask].detach().cpu().numpy()\n",
        "    #             accuracy = accuracy_score(pred_labels, true_labels)\n",
        "    #             val_loss = outputs.loss\n",
        "    #             val_accuracies.append(accuracy)\n",
        "    #             val_losses.append(val_loss.item())\n",
        "\n",
        "    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n",
        "         Train Loss: {sum(losses)/len(losses)}\\\n",
        "         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n",
        "         Val Loss: {sum(val_losses)/len(val_losses)}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "hops",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3308bc33b63b4b0892ec7040e82d8a23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6301d3e1abe34dcaa735069e598fb47c",
              "IPY_MODEL_4beefa2c38f44908ae3e7899280c0a36",
              "IPY_MODEL_fb344879a44442cf919a26fbc9eb2d41"
            ],
            "layout": "IPY_MODEL_c296e0cdf85f453d837dd3a23710a792"
          }
        },
        "6301d3e1abe34dcaa735069e598fb47c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ec753f5b60e4664aac81ceaead5eefd",
            "placeholder": "​",
            "style": "IPY_MODEL_0742a8befd4b469a9119078aa6a8ff99",
            "value": "  1%"
          }
        },
        "4beefa2c38f44908ae3e7899280c0a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2bdd1e2a60b4a09840d6d1aebebc7e5",
            "max": 136,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8b096dc421f4b2aa87b0d729ee7afa2",
            "value": 1
          }
        },
        "fb344879a44442cf919a26fbc9eb2d41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f370e11bf4a4b688a5add56c2f776e2",
            "placeholder": "​",
            "style": "IPY_MODEL_a5048b6496af40b3aa4d45924402d3f8",
            "value": " 1/136 [01:04&lt;1:51:42, 49.65s/it, Batch=1, Pixel-wise accuracy=0.644, Loss=0.651]"
          }
        },
        "c296e0cdf85f453d837dd3a23710a792": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ec753f5b60e4664aac81ceaead5eefd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0742a8befd4b469a9119078aa6a8ff99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2bdd1e2a60b4a09840d6d1aebebc7e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8b096dc421f4b2aa87b0d729ee7afa2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f370e11bf4a4b688a5add56c2f776e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5048b6496af40b3aa4d45924402d3f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}