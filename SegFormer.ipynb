{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based off of https://colab.research.google.com/drive/1_t3KvF3qg4IJfEhTuftFI1GSlscapNgf?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torchvision import transforms\n",
    "\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerFeatureExtractor\n",
    "from transformers import AdamW\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm.notebook import tqdm\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import albumentations as aug\n",
    "import numpy as np\n",
    "import glob\n",
    "import re\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = re.compile(r'(\\d+)')\n",
    "\n",
    "def numericalSort(value):\n",
    "    parts = numbers.split(value)\n",
    "    parts[1::2] = map(int, parts[1::2])\n",
    "    return parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your transformations\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "mask_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor()\n",
    "  \n",
    "])\n",
    "\n",
    "class SemanticSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, image_transform=image_transform, mask_transform=mask_transform):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.image_transform = image_transform\n",
    "        self.mask_transform = mask_transform\n",
    "      \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and mask\n",
    "        image = Image.open(self.image_paths[idx])\n",
    "        image = image.convert('RGB')\n",
    "        \n",
    "        mask = Image.open(self.mask_paths[idx])\n",
    "        mask = image.convert('L')\n",
    "\n",
    "         # Apply transformations\n",
    "        if self.image_transform:\n",
    "             image = self.image_transform(image)  \n",
    "             mask = self.mask_transform(mask)   \n",
    "\n",
    "        feature_extractor = SegformerFeatureExtractor(align=False, reduce_zero_label=False)\n",
    "        image_features = feature_extractor(image)\n",
    "        image_tensor = torch.tensor(image_features.pixel_values)\n",
    "        image_tensor = image_tensor.squeeze(0)\n",
    "\n",
    "        return image_tensor, mask\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r'E:/Hops/Corrected/256/Input'\n",
    "mask_path =  r'E:/Hops/Corrected/256/Target'\n",
    "\n",
    "images = sorted(glob.glob(image_path + '/*.png'), key = numericalSort)\n",
    "masks = sorted(glob.glob(mask_path + '/*.png'), key = numericalSort)\n",
    "\n",
    "dataset = SemanticSegmentationDataset(images, masks, image_transform=image_transform, mask_transform=mask_transform)\n",
    "\n",
    "test_split = 0.2\n",
    "shuffle_dataset = True\n",
    "random_seed = 42\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(test_split * dataset_size))\n",
    "\n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "train_dataloader = DataLoader(dataset, batch_size=2, sampler=train_sampler)\n",
    "test_dataloader = DataLoader(dataset, batch_size=2, sampler=test_sampler)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_sampler))\n",
    "print(\"Number of validation examples:\", len(test_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: 'background', 1: 'foreground'}\n",
    "label2id = {'background': 0, 'foreground': 1}\n",
    "\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/mit-b5\", ignore_mismatched_sizes=True,\n",
    "                                                         num_labels=len(id2label), id2label=id2label, label2id=label2id,\n",
    "                                                         reshape_last_stage=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.00006)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(\"Model Initialized!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "for epoch in range(1, 2):  # loop over the dataset multiple times\n",
    "    print(\"Epoch:\", epoch)\n",
    "    pbar = tqdm(train_dataloader)\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "    val_accuracies = []\n",
    "    val_losses = []\n",
    "    model.train()\n",
    "    for idx, batch in enumerate(pbar):\n",
    "        # get the inputs;\n",
    "        pixel_values = batch[0].to(device)\n",
    "        \n",
    "        labels = batch[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward\n",
    "        outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "\n",
    "        # evaluate\n",
    "        upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "        predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "        mask = (labels != 255) # we don't include the background class in the accuracy calculation\n",
    "        pred_labels = predicted[mask].detach().cpu().numpy()\n",
    "        true_labels = labels[mask].detach().cpu().numpy()\n",
    "        accuracy = accuracy_score(pred_labels, true_labels)\n",
    "        loss = outputs.loss\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_postfix({'Batch': idx, 'Pixel-wise accuracy': sum(accuracies)/len(accuracies), 'Loss': sum(losses)/len(losses)})\n",
    "\n",
    "        # backward + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # else:\n",
    "    #     model.eval()\n",
    "    #     with torch.no_grad():\n",
    "    #         for idx, batch in enumerate(valid_dataloader):\n",
    "    #             pixel_values = batch[\"pixel_values\"].to(device)\n",
    "    #             labels = batch[\"labels\"].to(device)\n",
    "\n",
    "    #             outputs = model(pixel_values=pixel_values, labels=labels)\n",
    "    #             upsampled_logits = nn.functional.interpolate(outputs.logits, size=labels.shape[-2:], mode=\"bilinear\", align_corners=False)\n",
    "    #             predicted = upsampled_logits.argmax(dim=1)\n",
    "\n",
    "    #             mask = (labels != 255) # we don't include the background class in the accuracy calculation\n",
    "    #             pred_labels = predicted[mask].detach().cpu().numpy()\n",
    "    #             true_labels = labels[mask].detach().cpu().numpy()\n",
    "    #             accuracy = accuracy_score(pred_labels, true_labels)\n",
    "    #             val_loss = outputs.loss\n",
    "    #             val_accuracies.append(accuracy)\n",
    "    #             val_losses.append(val_loss.item())\n",
    "\n",
    "    print(f\"Train Pixel-wise accuracy: {sum(accuracies)/len(accuracies)}\\\n",
    "         Train Loss: {sum(losses)/len(losses)}\\\n",
    "         Val Pixel-wise accuracy: {sum(val_accuracies)/len(val_accuracies)}\\\n",
    "         Val Loss: {sum(val_losses)/len(val_losses)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
